services:
  backend:
    restart: unless-stopped
    image: vv8_backend_api
    depends_on:
      - task_queue_broker
      - database
      - mongodb
    build:
      context: ./backend
      dockerfile: Dockerfile
      network: host
    ports:
      - "4000:4000/tcp"
    environment:
      VV8_CELERY_BROKER: task_queue_broker
      VV8_CELERY_BROKER_PORT: 6379
      VV8_CELERY_ID: vv8_web_server
      SQL_USERNAME: ${SQL_USER:-vv8}
      SQL_PASSWORD: ${SQL_PASSWORD:-vv8}
      SQL_HOST: ${SQL_HOST:-database}
      SQL_PORT: ${SQL_PORT:-5432}
      SQL_DATABASE: ${SQL_DB:-vv8_backend}
      MONGO_HOST: ${MONGO_HOST:-mongodb}
      MONGO_PORT: ${MONGO_PORT:-27017}
      MONGO_USER: ${MONGO_USER:-vv8}
      MONGO_PASSWORD: ${MONGO_PASSWORD:-vv8}
      MONGO_DATABASE: ${MONGO_DATABASE:-admin}

  database:
    restart: unless-stopped
    image: vv8_postgres
    build:
      context: ./vv8_backend_database
      dockerfile: Dockerfile
      network: host
    command: postgres -c config_file=/etc/postgresql.conf
    ports:
      - "5434:5432/tcp"
    environment:
      POSTGRES_PASSWORD: vv8
      POSTGRES_USER: vv8
      POSTGRES_DB: vv8_backend
      PGDATA: /var/lib/postgresql/data/pg_data
    volumes:
      - vv8postgresdb:/var/lib/postgresql/data

  task_queue_broker:
    restart: unless-stopped
    image: redis:6.2
    volumes:
      - redis_data:/data
    ports:
      - "6380:6379/tcp"

  redis_exporter:
    image: oliver006/redis_exporter:latest
    ports:
      - "9121:9121"  # Expose Redis Exporter metrics on port 9121
    environment:
      - REDIS_ADDR=172.17.0.1:6380  # Redis connection details
    depends_on:
      - task_queue_broker

  mongodb:
    image: mongo:4
    environment:
      MONGO_INITDB_ROOT_USERNAME: vv8
      MONGO_INITDB_ROOT_PASSWORD: vv8
    command: --wiredTigerCacheSizeGB 2.0
    ports:
      - "27019:27017/tcp"
    restart: unless-stopped
    volumes:
      - mongo_data:/data/db
      - mongo_init:/docker-entrypoint-initdb.d/:ro

  vv8_worker:
    restart: unless-stopped
    depends_on:
      - task_queue_broker
      - database
      - jaeger
      - prometheus
    ports:
      - "5901:5901/tcp"
      - "6901:6901/tcp"
      - "9464:9464/tcp"  # Prometheus metrics endpoint
    build:
      context: ./celery_workers
      dockerfile: vv8_worker.dockerfile
      network: host
    image: vv8_crawler_worker
    environment:
      VV8_CELERY_BROKER: task_queue_broker
      VV8_CELERY_BROKER_PORT: 6379
      VV8_CELERY_ID: vv8_worker
      CELERY_CONCURRENCY: ${CELERY_CONCURRENCY:-9}
      MONGO_HOST: ${MONGO_HOST:-mongodb}
      MONGO_PORT: ${MONGO_PORT:-27017}
      MONGO_USER: ${MONGO_USER:-vv8}
      MONGO_PASSWORD: ${MONGO_PASSWORD:-vv8}
      MONGO_DATABASE: ${MONGO_DATABASE:-admin}
      # OpenTelemetry configuration
      JAEGER_ENDPOINT: http://jaeger:14268/api/traces
      OTEL_SERVICE_NAME: web-crawler
      NODE_ENV: production
    volumes:
      - screenshots:/app/screenshots:rwx
      - har:/app/har:rwx
      - vv8_crawler:/app/node:rwx
      - /app/node/node_modules  # anonymous volume for node_modules
      - raw_logs:/app/vv8_worker/raw_logs:rwx
      - /var/run/docker.sock:/var/run/docker.sock  # For container discovery
    labels:
      - "com.prometheus.scrape=true"
      - "com.prometheus.port=9464"

  log_parser_worker:
    restart: unless-stopped
    depends_on:
      - task_queue_broker
      - database
      - mongodb
    build:
      context: ./celery_workers
      dockerfile: log_parser.dockerfile
      network: host
      args:
        DOCKER_IMAGE: visiblev8/vv8-postprocessors:latest
    image: vv8_log_parser_worker
    volumes:
      - raw_logs:/app/raw_logs:rwx
      - parsed_logs:/app/parsed_logs:rwx
    env_file:
      - .env
    environment:
      VV8_CELERY_BROKER: task_queue_broker
      VV8_CELERY_BROKER_PORT: 6379
      VV8_CELERY_ID: vv8_log_parser
      MONGODB_HOST: mongodb
      MONGODB_PORT: 27017
      MONGODB_USER: vv8
      MONGODB_PWD: vv8
      MONGODB_AUTHDB: admin
      CELERY_CONCURRENCY: ${CELERY_CONCURRENCY:-9}
      PGHOST: ${SQL_HOST:-database}
      PGPORT: ${SQL_PORT:-5432}
      PGUSER: ${SQL_USER:-vv8}
      PGPASSWORD: ${SQL_PASSWORD:-vv8}
      PGDATABASE: ${SQL_DATABASE:-vv8_backend}
      ADBLOCK_BINARY: /app/post-processors/adblock
      EASYPRIVACY_FILE: /app/post-processors/easyprivacy.txt
      EASYLIST_FILE: /app/post-processors/easylist.txt
      EMAP_FILE: /app/post-processors/entities.json
      IDLDATA_FILE: /artifacts/idldata.json

  flower:
    restart: unless-stopped
    depends_on:
      - task_queue_broker
      - database
    image: vv8-vv8_crawler_flower_monitor
    environment:
      - FLOWER_UNAUTHENTICATED_API=True
    build:
      context: ./flower
      dockerfile: Dockerfile
      network: host
    volumes:
      - flower_data:/etc/db:rwx
    ports:
      - "5555:5555/tcp"

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9091:9090/tcp"  # Changed to 9091 to avoid collision with Minio
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.enable-lifecycle"
      - "--storage.tsdb.retention.time=60d"

  loki:
    image: grafana/loki:3.4.1
    container_name: loki
    ports:
      - "3100:3100"
    command: -config.file=/mnt/config/loki-config.yaml
    volumes:
      - loki_home:/mnt/config
      - loki_data:/tmp/loki

  # Promtail to scrape Docker container logs
  promtail:
    image: grafana/promtail:3.4.1
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - loki_home:/mnt/config
      - loki_data:/tmp/loki
    command: -config.file=/mnt/config/promtail-config.yaml      
    depends_on:
      - loki

  
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000/tcp"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
      - loki

  jaeger:
    image: jaegertracing/all-in-one:1.60.0
    container_name: jaeger
    restart: unless-stopped
    ports:
      - "16686:16686/tcp"  # Jaeger UI
      - "6831:6831/udp"   # Jaeger agent (UDP)
      - "14268:14268/tcp"  # Jaeger collector (HTTP)
      - "14250:14250/tcp"  # Jaeger collector (gRPC)
      - "14318:4318"
      - "14317:4371"
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=9411
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key

    volumes:
      - jaeger_data:/badger

  # New service: OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib
    ports:
      - 1888:1888 # pprof extension
      - 8888:8888 # Prometheus metrics exposed by the Collector
      - 8889:8889 # Prometheus exporter metrics
      - 13133:13133 # health_check extension
      - 4317:4317 # OTLP gRPC receiver
      - 4318:4318 # OTLP http receiver
      - 55679:55679 # zpages extension
    volumes:
      - ./otel-collector/otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml
    command: ["--config=/etc/otelcol-contrib/config.yaml"]
    depends_on:
      - jaeger
      - prometheus

volumes:
  vv8postgresdb:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/vv8db2"
  redis_data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/redis_data"
  mongo_data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/mongo/data"
  mongo_init:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/mongo/init"
  screenshots:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/screenshots"
  har:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/har"
  vv8_crawler:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/celery_workers/vv8_worker/vv8_crawler"
  raw_logs:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/raw_logs"
  parsed_logs:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/parsed_logs"
  flower_data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/flower/data"
  grafana-storage:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/graphana"
  prometheus_data:
    driver: local
  jaeger_data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/jaeger"
  loki_data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/loki/data"
  loki_home:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.152.3,rw
      device: ":/u1/a8tariq/visiblev8-crawler/loki"