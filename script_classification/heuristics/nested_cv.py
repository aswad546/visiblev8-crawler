import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
import psycopg2
import psycopg2.extras
from sklearn.model_selection import (train_test_split, cross_val_score, 
                                   StratifiedKFold, GridSearchCV, 
                                   RandomizedSearchCV)
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, roc_curve, precision_recall_curve, 
                           average_precision_score, make_scorer)
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFECV
from sklearn.calibration import calibration_curve, CalibratedClassifierCV
from sklearn.base import clone
import pickle
import os
from datetime import datetime
from scipy import stats
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

class RobustBalancedVsImbalancedComparison:
    """
    Methodologically rigorous comparison of balanced vs imbalanced approaches
    using nested cross-validation to avoid hyperparameter selection bias.
    
    Key improvements:
    1. Nested cross-validation for unbiased performance estimation
    2. Hold-out test set for final evaluation
    3. Statistical significance testing
    4. Feature selection within CV folds
    5. Hyperparameter tuning within inner CV loop
    """
    
    def __init__(self, db_config=None, output_dir="robust_comparison_analysis"):
        self.db_config = db_config or {
            'host': 'localhost',
            'database': 'vv8_backend',
            'user': 'vv8',
            'password': 'vv8',
            'port': 5434
        }

        self.forced_test_ids = {
            7408029,                                 # Datadome
            7417129, 7412576, 7410200, 7413032,      # Cheq
            7406593, 7404903,                        # Feedzai
            7402914,                                 # Threatmark
            7413785, 7413660, 7415157, 7415812,
            7416649, 7418418, 7411575, 7412158,      # Yofi
            7414903,                                 # Group‚ÄëIB
            7412043,                                 # Untarget
            7413095                                  # Callsign
        }
        
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        self.table_name = 'multicore_static_info_known_companies'
        self.raw_data = None
        
        # Data storage
        self.balanced_data = None
        self.imbalanced_data = None
        
        # Train/test splits for both approaches
        self.balanced_train = None
        self.balanced_test = None
        self.imbalanced_train = None
        self.imbalanced_test = None
        
        # Results storage
        self.nested_cv_results = {
            'balanced': None,
            'imbalanced': None
        }
        
        # Hyperparameter search space
        self.param_distributions = {
            'n_estimators': [100, 200, 300, 400],
            'max_depth': [8, 10, 12, 15, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', 0.3, 0.5],
            'bootstrap': [True, False]
        }
    
    def connect_to_database(self):
        """Establish connection to PostgreSQL database."""
        try:
            connection = psycopg2.connect(**self.db_config)
            return connection
        except psycopg2.Error as e:
            print(f"‚ùå Error connecting to PostgreSQL database: {e}")
            return None
        
    # def load_and_split_datasets(self, test_size: float = 0.2, random_state: int = 42):
    #     """
    #     Fixed‚Äëpositive / proportional‚Äënegative train‚Äëtest split.

    #     ‚Ä¢ Test positives  : *exactly* the `script_id`s in `self.forced_test_ids`
    #                         (must have label == 1).
    #     ‚Ä¢ Test negatives  : random sample of size  `test_size` √ó (# total negatives)
    #                         drawn from the remaining negatives.
    #     ‚Ä¢ All other samples go to training.

    #     Two views are produced:
    #     ‚Äì balanced   : keep labels {0, 1} only
    #     ‚Äì imbalanced : map  1 ‚Üí 1 , {0,‚Äë1} ‚Üí 0
    #     """
    #     import random
    #     from sklearn.model_selection import train_test_split

    #     # -------------------------------------------------------------- #
    #     # 1.  LOAD everything (same logic as before, incl. JSON parsing)
    #     # -------------------------------------------------------------- #
    #     print("üîå Loading data and applying custom split ...")
    #     conn = self.connect_to_database()
    #     if conn is None:
    #         raise RuntimeError("Failed to connect to database")

    #     with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
    #         cur.execute(f"SELECT * FROM {self.table_name}")
    #         rows = cur.fetchall()
    #     conn.close()

    #     self.raw_data = []
    #     json_fields = [
    #         "aggregated_behavioral_apis", "aggregated_fingerprinting_apis",
    #         "fingerprinting_source_apis", "behavioral_source_apis",
    #         "behavioral_apis_access_count", "fingerprinting_api_access_count",
    #         "apis_going_to_sink", "max_aggregated_apis",
    #     ]

    #     for row in rows:
    #         rec = dict(row)
    #         for fld in json_fields:
    #             if fld in rec and rec[fld] is not None and isinstance(rec[fld], str):
    #                 try:
    #                     rec[fld] = json.loads(rec[fld])
    #                 except json.JSONDecodeError:
    #                     rec[fld] = None
    #         self.raw_data.append(rec)

    #     print(f"‚úÖ Loaded {len(self.raw_data)} total scripts")

    #     # -------------------------------------------------------------- #
    #     # 2.  Identify forced‚Äëpositive indices
    #     # -------------------------------------------------------------- #
    #     id2idx = {s["script_id"]: i for i, s in enumerate(self.raw_data)}
    #     forced_pos_idx = [
    #         id2idx[sid] for sid in self.forced_test_ids
    #         if sid in id2idx and self.raw_data[id2idx[sid]]["label"] == 1
    #     ]

    #     missing = self.forced_test_ids - {self.raw_data[i]["script_id"] for i in forced_pos_idx}
    #     if missing:
    #         print(f"‚ö†Ô∏è  {len(missing)} of the forced IDs not found as label‚Äë1: {sorted(list(missing))[:10]}")

    #     # -------------------------------------------------------------- #
    #     # 3.  Build candidate negative pool (label 0 or ‚Äë1, not forced)
    #     # -------------------------------------------------------------- #
    #     neg_pool_idx = [
    #         i for i, s in enumerate(self.raw_data)
    #         if (s["label"] in (0, -1)) and (s["script_id"] not in self.forced_test_ids)
    #     ]

    #     # sample proportionate negatives
    #     rng = random.Random(random_state)
    #     rng.shuffle(neg_pool_idx)  # reproducible shuffle
    #     n_test_neg = int(len(neg_pool_idx) * test_size)
    #     test_neg_idx = neg_pool_idx[:n_test_neg]
    #     train_neg_idx = neg_pool_idx[n_test_neg:]

    #     # -------------------------------------------------------------- #
    #     # 4.  Remaining positives (not forced) go entirely to training
    #     # -------------------------------------------------------------- #
    #     other_pos_idx = [
    #         i for i, s in enumerate(self.raw_data)
    #         if (s["label"] == 1) and (s["script_id"] not in self.forced_test_ids)
    #     ]

    #     # -------------------------------------------------------------- #
    #     # 5.  Assemble final index lists
    #     # -------------------------------------------------------------- #
    #     test_idx  = forced_pos_idx + test_neg_idx
    #     train_idx = other_pos_idx + train_neg_idx

    #     # -------------------------------------------------------------- #
    #     # 6.  BALANCED view   (labels 0 / 1 only)
    #     # -------------------------------------------------------------- #
    #     self.balanced_train = [
    #         self.raw_data[i] for i in train_idx
    #         if self.raw_data[i]["label"] in (0, 1)
    #     ]
    #     self.balanced_test = [
    #         self.raw_data[i] for i in test_idx
    #         if self.raw_data[i]["label"] in (0, 1)
    #     ]

    #     print("\nüéØ BALANCED Split:")
    #     print(f"  Train : {len(self.balanced_train)} samples")
    #     print(f"  Test  : {len(self.balanced_test)} samples "
    #         f"({sum(s['label'] == 1 for s in self.balanced_test)} positives)")

    #     # -------------------------------------------------------------- #
    #     # 7.  IMBALANCED view  (map 0/‚Äë1 ‚Üí 0)
    #     # -------------------------------------------------------------- #
    #     def relabel(sample):
    #         x = sample.copy()
    #         x["label"] = 1 if x["label"] == 1 else 0
    #         return x

    #     self.imbalanced_train = [relabel(self.raw_data[i]) for i in train_idx]
    #     self.imbalanced_test  = [relabel(self.raw_data[i]) for i in test_idx]

    #     pos_train = sum(s["label"] for s in self.imbalanced_train)
    #     neg_train = len(self.imbalanced_train) - pos_train

    #     print("\n‚öñÔ∏è  IMBALANCED Split:")
    #     print(f"  Train : {len(self.imbalanced_train)} samples "
    #         f"(ratio 1:{neg_train/pos_train:.1f})")
    #     print(f"  Test  : {len(self.imbalanced_test)} samples "
    #         f"({sum(s['label'] for s in self.imbalanced_test)} positives)")

    #     print("\nüöÄ Custom split complete ‚Äî ready for nested CV.")

    
    def load_and_split_datasets(self, test_size=0.2, random_state=42):
        """
        Load data and create train/test splits for both approaches.
        
        Critical: The test set is held out and NEVER used during model selection.
        """
        print("üîå Loading data and creating train/test splits...")
        
        # Connect to database
        connection = self.connect_to_database()
        if connection is None:
            raise Exception("Failed to connect to database")
        
        try:
            # Query to fetch all data
            query = f"SELECT * FROM {self.table_name}"
            cursor = connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            cursor.execute(query)
            rows = cursor.fetchall()
            
            # Convert to list of dictionaries with JSON parsing
            self.raw_data = []
            
            for row in rows:
                record = dict(row)
                
                # Parse JSON fields
                json_fields = [
                    'aggregated_behavioral_apis', 'aggregated_fingerprinting_apis',
                    'fingerprinting_source_apis', 'behavioral_source_apis',
                    'behavioral_apis_access_count', 'fingerprinting_api_access_count',
                    'apis_going_to_sink', 'max_aggregated_apis'
                ]
                
                for field in json_fields:
                    if field in record and record[field] is not None:
                        if isinstance(record[field], str):
                            try:
                                record[field] = json.loads(record[field])
                            except json.JSONDecodeError:
                                record[field] = None
                        elif record[field] == '{}':
                            record[field] = {}
                        elif record[field] == '[]':
                            record[field] = []
                
                self.raw_data.append(record)
            
            cursor.close()
            
        finally:
            connection.close()
        
        print(f"‚úÖ Loaded {len(self.raw_data)} total scripts")
        
        # Prepare BALANCED dataset
        balanced_data = [script for script in self.raw_data if script['label'] in [0, 1]]
        
        # Create stratified train/test split
        balanced_labels = [script['label'] for script in balanced_data]
        balanced_indices = list(range(len(balanced_data)))
        
        train_idx, test_idx = train_test_split(
            balanced_indices, 
            test_size=test_size,
            stratify=balanced_labels,
            random_state=random_state
        )
        
        self.balanced_train = [balanced_data[i] for i in train_idx]
        self.balanced_test = [balanced_data[i] for i in test_idx]
        
        print(f"\nüéØ BALANCED Dataset Split:")
        print(f"  Training: {len(self.balanced_train)} samples")
        print(f"  Testing: {len(self.balanced_test)} samples (held-out)")
        
        # Prepare IMBALANCED dataset
        imbalanced_data = []
        for script in self.raw_data:
            script_copy = script.copy()
            if script['label'] == 1:
                script_copy['label'] = 1
            else:  # labels 0 and -1
                script_copy['label'] = 0
            imbalanced_data.append(script_copy)
        
        # Create stratified train/test split
        imbalanced_labels = [script['label'] for script in imbalanced_data]
        imbalanced_indices = list(range(len(imbalanced_data)))
        
        train_idx, test_idx = train_test_split(
            imbalanced_indices,
            test_size=test_size,
            stratify=imbalanced_labels,
            random_state=random_state
        )
        
        self.imbalanced_train = [imbalanced_data[i] for i in train_idx]
        self.imbalanced_test = [imbalanced_data[i] for i in test_idx]
        
        print(f"\n‚öñÔ∏è  IMBALANCED Dataset Split:")
        print(f"  Training: {len(self.imbalanced_train)} samples")
        print(f"  Testing: {len(self.imbalanced_test)} samples (held-out)")
        
        # Show class distributions
        train_pos = sum([s['label'] for s in self.imbalanced_train])
        train_neg = len(self.imbalanced_train) - train_pos
        
        print(f"  Training ratio: 1:{train_neg/train_pos:.1f} (pos:neg)")
    
    def engineer_features(self, dataset, remove_correlated=True, correlation_threshold=0.9):
        """
        Engineer features with optional correlation removal.
        
        Important: This should be called within CV folds during training.
        """
        features_list = []
        
        for script in dataset:
            try:
                features = {}
                
                # Core aggregation features
                features['max_api_aggregation_score'] = script.get('max_api_aggregation_score', -1)
                features['behavioral_api_agg_count'] = script.get('behavioral_api_agg_count', -1)
                features['fp_api_agg_count'] = script.get('fp_api_agg_count', -1)
                
                # Volume indicators
                features['behavioral_source_api_count'] = script.get('behavioral_source_api_count', 0)
                features['fingerprinting_source_api_count'] = script.get('fingerprinting_source_api_count', 0)
                
                # Data flow indicators
                features['dataflow_to_sink'] = int(script.get('dataflow_to_sink', False))
                
                # API access intensity
                behavioral_access = script.get('behavioral_apis_access_count') or {}
                fp_access = script.get('fingerprinting_api_access_count') or {}
                
                features['total_behavioral_api_accesses'] = sum(behavioral_access.values())
                features['total_fp_api_accesses'] = sum(fp_access.values())
                features['unique_behavioral_apis'] = len(behavioral_access)
                features['unique_fp_apis'] = len(fp_access)
                
                # Sink analysis
                sink_data = script.get('apis_going_to_sink') or {}
                features['num_sink_types'] = len(sink_data)
                features['has_storage_sink'] = int(any('Storage' in str(sink) for sink in sink_data.keys()))
                features['has_network_sink'] = int(any(sink in ['XMLHttpRequest.send', 'Navigator.sendBeacon', 'fetch'] 
                                                      for sink in sink_data.keys()))
                
                # Behavioral event diversity
                behavioral_sources = script.get('behavioral_source_apis') or []
                features['mouse_event_count'] = sum(1 for api in behavioral_sources if 'MouseEvent' in str(api))
                features['keyboard_event_count'] = sum(1 for api in behavioral_sources if 'KeyboardEvent' in str(api))
                features['touch_event_count'] = sum(1 for api in behavioral_sources if 'TouchEvent' in str(api) or 'Touch.' in str(api))
                features['pointer_event_count'] = sum(1 for api in behavioral_sources if 'PointerEvent' in str(api))
                features['behavioral_event_diversity'] = sum([
                    features['mouse_event_count'] > 0,
                    features['keyboard_event_count'] > 0,
                    features['touch_event_count'] > 0,
                    features['pointer_event_count'] > 0
                ])
                
                # Ratio features
                total_apis = features['behavioral_source_api_count'] + features['fingerprinting_source_api_count']
                if total_apis > 0:
                    features['behavioral_ratio'] = features['behavioral_source_api_count'] / total_apis
                    features['intensity_ratio'] = features['total_behavioral_api_accesses'] / total_apis
                else:
                    features['behavioral_ratio'] = 0
                    features['intensity_ratio'] = 0
                
                # Store metadata
                features['script_id'] = script.get('script_id')
                features['label'] = script['label']
                
                features_list.append(features)
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Feature extraction error: {e}")
        
        # Create DataFrame
        features_df = pd.DataFrame(features_list)
        
        # Remove highly correlated features if requested
        if remove_correlated and len(features_df) > 10:
            metadata_cols = ['script_id', 'label']
            feature_cols = [col for col in features_df.columns if col not in metadata_cols]
            
            # Calculate correlation matrix
            corr_matrix = features_df[feature_cols].corr()
            
            # Find features to remove
            features_to_remove = set()
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    if abs(corr_matrix.iloc[i, j]) > correlation_threshold:
                        features_to_remove.add(corr_matrix.columns[j])
            
            # Keep only uncorrelated features
            final_features = [col for col in feature_cols if col not in features_to_remove]
            features_df = features_df[final_features + metadata_cols]
        
        return features_df
    
    def nested_cross_validation(self, train_data, dataset_name, 
                              outer_cv_folds=5, inner_cv_folds=3,
                              scoring='roc_auc', n_jobs=-1):
        """
        Perform nested cross-validation with hyperparameter tuning.
        
        This is the methodologically correct approach that avoids overfitting
        to the validation set during hyperparameter selection.
        """
        print(f"\nüîÑ Starting Nested Cross-Validation for {dataset_name} Dataset...")
        print(f"  Outer CV: {outer_cv_folds} folds (for performance estimation)")
        print(f"  Inner CV: {inner_cv_folds} folds (for hyperparameter tuning)")
        
        # Prepare features
        features_df = self.engineer_features(train_data)
        feature_cols = [col for col in features_df.columns if col not in ['script_id', 'label']]
        
        X = features_df[feature_cols].values
        y = features_df['label'].values
        
        # Initialize results storage
        outer_scores = {
            'train_acc': [], 'val_acc': [], 'train_auc': [], 'val_auc': [],
            'train_ap': [], 'val_ap': [], 'best_params': [], 'feature_importances': []
        }
        
        # Outer CV loop
        outer_cv = StratifiedKFold(n_splits=outer_cv_folds, shuffle=True, random_state=42)
        
        for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X, y)):
            print(f"\n  üìÅ Outer Fold {fold_idx + 1}/{outer_cv_folds}")
            
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # Inner CV for hyperparameter tuning
            inner_cv = StratifiedKFold(n_splits=inner_cv_folds, shuffle=True, random_state=42)
            
            # Configure RandomizedSearchCV for efficiency
            rf = RandomForestClassifier(
                class_weight='balanced',
                random_state=42,
                n_jobs=n_jobs
            )
            
            # Use RandomizedSearchCV for faster tuning
            random_search = RandomizedSearchCV(
                estimator=rf,
                param_distributions=self.param_distributions,
                n_iter=30,  # Number of parameter combinations to try
                cv=inner_cv,
                scoring=scoring,
                n_jobs=n_jobs,
                random_state=42,
                verbose=0
            )
            
            # Fit on inner CV
            print(f"    üîç Hyperparameter tuning on inner CV...")
            random_search.fit(X_train, y_train)
            
            # Best parameters from inner CV
            best_params = random_search.best_params_
            best_score = random_search.best_score_
            print(f"    ‚úÖ Best inner CV score: {best_score:.3f}")
            print(f"    üìã Best parameters: {best_params}")
            
            # Train final model for this fold with best parameters
            final_model = RandomForestClassifier(
                **best_params,
                class_weight='balanced',
                random_state=42,
                n_jobs=n_jobs
            )
            final_model.fit(X_train, y_train)
            
            # Evaluate on outer fold validation set
            train_pred = final_model.predict(X_train)
            val_pred = final_model.predict(X_val)
            train_proba = final_model.predict_proba(X_train)[:, 1]
            val_proba = final_model.predict_proba(X_val)[:, 1]
            
            # Calculate metrics
            train_acc = np.mean(train_pred == y_train)
            val_acc = np.mean(val_pred == y_val)
            train_auc = roc_auc_score(y_train, train_proba)
            val_auc = roc_auc_score(y_val, val_proba)
            train_ap = average_precision_score(y_train, train_proba)
            val_ap = average_precision_score(y_val, val_proba)
            
            print(f"    üìä Outer fold validation AUC: {val_auc:.3f}")
            
            # Store results
            outer_scores['train_acc'].append(train_acc)
            outer_scores['val_acc'].append(val_acc)
            outer_scores['train_auc'].append(train_auc)
            outer_scores['val_auc'].append(val_auc)
            outer_scores['train_ap'].append(train_ap)
            outer_scores['val_ap'].append(val_ap)
            outer_scores['best_params'].append(best_params)
            outer_scores['feature_importances'].append(final_model.feature_importances_)
        
        # Calculate summary statistics
        results = {
            'dataset_name': dataset_name,
            'feature_names': feature_cols,
            'outer_scores': outer_scores,
            'metrics': {
                'train_acc': {'mean': np.mean(outer_scores['train_acc']), 
                             'std': np.std(outer_scores['train_acc'])},
                'val_acc': {'mean': np.mean(outer_scores['val_acc']), 
                           'std': np.std(outer_scores['val_acc'])},
                'train_auc': {'mean': np.mean(outer_scores['train_auc']), 
                             'std': np.std(outer_scores['train_auc'])},
                'val_auc': {'mean': np.mean(outer_scores['val_auc']), 
                           'std': np.std(outer_scores['val_auc'])},
                'train_ap': {'mean': np.mean(outer_scores['train_ap']), 
                            'std': np.std(outer_scores['train_ap'])},
                'val_ap': {'mean': np.mean(outer_scores['val_ap']), 
                          'std': np.std(outer_scores['val_ap'])}
            }
        }
        
        print(f"\nüìä {dataset_name} Nested CV Results:")
        print(f"  Validation AUC: {results['metrics']['val_auc']['mean']:.3f} ¬± {results['metrics']['val_auc']['std']:.3f}")
        print(f"  Validation AP:  {results['metrics']['val_ap']['mean']:.3f} ¬± {results['metrics']['val_ap']['std']:.3f}")
        print(f"  Validation Acc: {results['metrics']['val_acc']['mean']:.3f} ¬± {results['metrics']['val_acc']['std']:.3f}")
        
        return results
    
    # def train_final_model(self, train_data, test_data, nested_cv_results, dataset_name):
    #     """
    #     Train a final model using the most common best parameters from nested CV
    #     and evaluate on the held-out test set.
    #     """
    #     print(f"\nüéØ Training final {dataset_name} model and evaluating on test set...")
        
    #     # Get the most common hyperparameters from nested CV
    #     from collections import Counter
        
    #     # Convert parameter dictionaries to tuples for counting
    #     param_tuples = [tuple(sorted(params.items())) 
    #                    for params in nested_cv_results['outer_scores']['best_params']]
        
    #     # Find most common parameter set
    #     most_common_params = Counter(param_tuples).most_common(1)[0][0]
    #     best_params = dict(most_common_params)
        
    #     print(f"  üìã Using most common parameters from nested CV: {best_params}")
        
    #     # Prepare train and test features
    #     train_features_df = self.engineer_features(train_data)
    #     test_features_df = self.engineer_features(test_data)
        
    #     # Ensure same features
    #     feature_cols = nested_cv_results['feature_names']
        
    #     X_train = train_features_df[feature_cols].values
    #     y_train = train_features_df['label'].values
    #     X_test = test_features_df[feature_cols].values
    #     y_test = test_features_df['label'].values
        
    #     # Train final model
    #     final_model = RandomForestClassifier(
    #         **best_params,
    #         class_weight='balanced',
    #         random_state=42,
    #         n_jobs=-1
    #     )
        
    #     final_model.fit(X_train, y_train)
        
    #     # Evaluate on test set
    #     test_pred = final_model.predict(X_test)
    #     test_proba = final_model.predict_proba(X_test)[:, 1]
        
    #     test_acc = np.mean(test_pred == y_test)
    #     test_auc = roc_auc_score(y_test, test_proba)
    #     test_ap = average_precision_score(y_test, test_proba)
        
    #     # Calculate additional metrics
    #     tn, fp, fn, tp = confusion_matrix(y_test, test_pred).ravel()
    #     precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    #     recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    #     specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
    #     results = {
    #         'model': final_model,
    #         'params': best_params,
    #         'test_metrics': {
    #             'accuracy': test_acc,
    #             'auc': test_auc,
    #             'average_precision': test_ap,
    #             'precision': precision,
    #             'recall': recall,
    #             'specificity': specificity,
    #             'confusion_matrix': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}
    #         },
    #         'feature_importance': pd.DataFrame({
    #             'feature': feature_cols,
    #             'importance': final_model.feature_importances_
    #         }).sort_values('importance', ascending=False)
    #     }
        
    #     print(f"\nüìä {dataset_name} Test Set Performance:")
    #     print(f"  Test AUC: {test_auc:.3f}")
    #     print(f"  Test AP:  {test_ap:.3f}")
    #     print(f"  Test Acc: {test_acc:.3f}")
    #     print(f"  Precision: {precision:.3f}")
    #     print(f"  Recall: {recall:.3f}")
    #     print(f"  Specificity: {specificity:.3f}")
        
    #     return results
    
    def train_final_model(self, train_data, test_data, nested_cv_results, dataset_name):
        """
        Train a final model using the most common best parameters from nested CV
        and evaluate on the held-out test set.
        """
        print(f"\nüéØ Training final {dataset_name} model and evaluating on test set...")
        
        # 1) Get the most common hyperparameters from nested CV
        from collections import Counter
        param_tuples = [
            tuple(sorted(params.items()))
            for params in nested_cv_results['outer_scores']['best_params']
        ]
        best_params = dict(Counter(param_tuples).most_common(1)[0][0])
        print(f"  üìã Using most common parameters from nested CV: {best_params}")
        
        # 2) Engineer features **with correlation-dropping** on both train & test
        train_df = self.engineer_features(train_data, remove_correlated=True)
        test_df  = self.engineer_features(test_data,  remove_correlated=True)
        
        # 3) Align columns exactly
        feature_cols = nested_cv_results['feature_names']
        # reindex both so they have exactly feature_cols + ['label','script_id']
        train_df = train_df.reindex(
            columns=feature_cols + ['label', 'script_id'], 
            fill_value=0
        )
        test_df  = test_df.reindex(
            columns=feature_cols + ['label', 'script_id'], 
            fill_value=0
        )
        
        # 4) Slice out X / y
        X_train, y_train = train_df[feature_cols].values, train_df['label'].values
        X_test,  y_test  = test_df[ feature_cols].values, test_df['label'].values
        
        # 5) Train final RandomForest
        final_model = RandomForestClassifier(
            **best_params,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        final_model.fit(X_train, y_train)
        
        # 6) Evaluate on test set
        test_pred  = final_model.predict(X_test)
        test_proba = final_model.predict_proba(X_test)[:, 1]
        
        test_acc = np.mean(test_pred == y_test)
        test_auc = roc_auc_score(y_test, test_proba)
        test_ap  = average_precision_score(y_test, test_proba)
        
        tn, fp, fn, tp = confusion_matrix(y_test, test_pred).ravel()
        precision   = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall      = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        # 7) Package results
        results = {
            'model': final_model,
            'params': best_params,
            'test_metrics': {
                'accuracy': test_acc,
                'auc': test_auc,
                'average_precision': test_ap,
                'precision': precision,
                'recall': recall,
                'specificity': specificity,
                'confusion_matrix': {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp}
            },
            'feature_importance': pd.DataFrame({
                'feature': feature_cols,
                'importance': final_model.feature_importances_
            }).sort_values('importance', ascending=False)
        }
        
        print(f"\nüìä {dataset_name} Test Set Performance:")
        print(f"  Test AUC: {test_auc:.3f}")
        print(f"  Test AP:  {test_ap:.3f}")
        print(f"  Test Acc: {test_acc:.3f}")
        print(f"  Precision: {precision:.3f}")
        print(f"  Recall:    {recall:.3f}")
        print(f"  Specificity: {specificity:.3f}")
        
        return results


    def statistical_comparison(self, balanced_scores, imbalanced_scores):
        """
        Perform statistical significance testing between the two approaches.
        """
        print(f"\nüìä Statistical Comparison of Models...")
        
        results = {}
        
        # Paired t-test for each metric
        metrics = ['val_auc', 'val_ap', 'val_acc']
        
        for metric in metrics:
            balanced_vals = balanced_scores['outer_scores'][metric]
            imbalanced_vals = imbalanced_scores['outer_scores'][metric]
            
            # Paired t-test (same CV folds)
            t_stat, p_value = stats.ttest_rel(balanced_vals, imbalanced_vals)
            
            # Cohen's d for effect size
            diff = np.array(imbalanced_vals) - np.array(balanced_vals)
            cohen_d = np.mean(diff) / np.std(diff)
            
            # Wilcoxon signed-rank test (non-parametric alternative)
            wilcoxon_stat, wilcoxon_p = stats.wilcoxon(balanced_vals, imbalanced_vals)
            
            results[metric] = {
                'balanced_mean': np.mean(balanced_vals),
                'imbalanced_mean': np.mean(imbalanced_vals),
                'difference': np.mean(imbalanced_vals) - np.mean(balanced_vals),
                't_statistic': t_stat,
                'p_value': p_value,
                'cohen_d': cohen_d,
                'wilcoxon_p': wilcoxon_p,
                'significant': p_value < 0.05
            }
            
            print(f"\n  {metric.upper()}:")
            print(f"    Balanced: {results[metric]['balanced_mean']:.3f}")
            print(f"    Imbalanced: {results[metric]['imbalanced_mean']:.3f}")
            print(f"    Difference: {results[metric]['difference']:.3f}")
            print(f"    p-value: {results[metric]['p_value']:.4f}")
            print(f"    Cohen's d: {results[metric]['cohen_d']:.3f}")
            print(f"    Significant: {'Yes' if results[metric]['significant'] else 'No'}")
        
        return results
    
    def run_robust_comparison(self):
        """
        Run the complete methodologically rigorous comparison.
        """
        print("üöÄ " + "="*70)
        print("ROBUST MODEL COMPARISON WITH NESTED CROSS-VALIDATION")
        print("="*70)
        
        # Step 1: Load and split data
        print(f"\nüìö STEP 1: Load and Split Data")
        self.load_and_split_datasets(test_size=0.2)

        print("\nüìå Final Held-Out Test Set ‚Äî Positive Scripts")

        # BALANCED
        print("\nüîπ Balanced Test Set (label == 1):")
        for s in self.balanced_test:
            if s['label'] == 1:
                print(f"  script_id: {s.get('script_id')}, script_url: {s.get('script_url')}")

        # IMBALANCED
        # (optional, since it's a superset of balanced positives)
        # Uncomment below if you'd like both versions:
        print("\nüîπ Imbalanced Test Set (label == 1):")
        for s in self.imbalanced_test:
            if s['label'] == 1:
                print(f"  script_id: {s.get('script_id')}, script_url: {s.get('script_url')}")
        
        # Step 2: Nested cross-validation for both approaches
        print(f"\nüîÑ STEP 2: Nested Cross-Validation")
        
        self.nested_cv_results['balanced'] = self.nested_cross_validation(
            self.balanced_train, "Balanced"
        )
        
        self.nested_cv_results['imbalanced'] = self.nested_cross_validation(
            self.imbalanced_train, "Imbalanced"
        )
        
        # Step 3: Statistical comparison
        print(f"\nüìä STEP 3: Statistical Comparison")
        stat_comparison = self.statistical_comparison(
            self.nested_cv_results['balanced'],
            self.nested_cv_results['imbalanced']
        )
        
        # Step 4: Train final models and test
        print(f"\nüéØ STEP 4: Final Model Training and Testing")
        
        balanced_final = self.train_final_model(
            self.balanced_train,
            self.balanced_test,
            self.nested_cv_results['balanced'],
            "Balanced"
        )
        
        imbalanced_final = self.train_final_model(
            self.imbalanced_train,
            self.imbalanced_test,
            self.nested_cv_results['imbalanced'],
            "Imbalanced"
        )
        
        # Step 5: Generate comprehensive report
        print(f"\nüìù STEP 5: Generate Comprehensive Report")
        self.generate_robust_report(
            self.nested_cv_results,
            stat_comparison,
            balanced_final,
            imbalanced_final
        )
        
        # Step 6: Create visualizations
        print(f"\nüìä STEP 6: Create Visualizations")
        self.create_robust_visualizations(
            self.nested_cv_results,
            stat_comparison,
            balanced_final,
            imbalanced_final
        )
        
        print("\n‚úÖ Robust comparison complete!")
        
        return {
            'nested_cv_results': self.nested_cv_results,
            'statistical_comparison': stat_comparison,
            'balanced_final': balanced_final,
            'imbalanced_final': imbalanced_final
        }
    
    def generate_robust_report(self, nested_cv_results, stat_comparison, 
                              balanced_final, imbalanced_final):
        """
        Generate a methodologically rigorous report for thesis.
        """
        report_file = f"{self.output_dir}/robust_comparison_report_{self.timestamp}.md"
        
        with open(report_file, 'w') as f:
            f.write("# Robust Comparison of Balanced vs Imbalanced Approaches\n\n")
            f.write("## Methodology\n\n")
            f.write("This analysis uses nested cross-validation to avoid hyperparameter selection bias:\n\n")
            f.write("- **Outer CV**: 5-fold for unbiased performance estimation\n")
            f.write("- **Inner CV**: 3-fold for hyperparameter tuning\n")
            f.write("- **Test Set**: 20% held-out data never seen during model selection\n")
            f.write("- **Statistical Testing**: Paired t-test and Wilcoxon signed-rank test\n\n")
            
            f.write("## Key Results\n\n")
            
            # Nested CV results
            f.write("### Nested Cross-Validation Performance\n\n")
            f.write("| Metric | Balanced | Imbalanced | p-value | Significant |\n")
            f.write("|--------|----------|------------|---------|-------------|\n")
            
            for metric in ['val_auc', 'val_ap', 'val_acc']:
                bal_mean = nested_cv_results['balanced']['metrics'][metric]['mean']
                bal_std = nested_cv_results['balanced']['metrics'][metric]['std']
                imb_mean = nested_cv_results['imbalanced']['metrics'][metric]['mean']
                imb_std = nested_cv_results['imbalanced']['metrics'][metric]['std']
                p_val = stat_comparison[metric]['p_value']
                sig = "Yes" if stat_comparison[metric]['significant'] else "No"
                
                f.write(f"| {metric} | {bal_mean:.3f}¬±{bal_std:.3f} | "
                       f"{imb_mean:.3f}¬±{imb_std:.3f} | {p_val:.4f} | {sig} |\n")
            
            f.write("\n### Hold-out Test Set Performance\n\n")
            f.write("| Metric | Balanced | Imbalanced |\n")
            f.write("|--------|----------|------------|\n")
            
            test_metrics = ['auc', 'average_precision', 'accuracy', 'precision', 'recall']
            for metric in test_metrics:
                bal_val = balanced_final['test_metrics'][metric]
                imb_val = imbalanced_final['test_metrics'][metric]
                f.write(f"| {metric} | {bal_val:.3f} | {imb_val:.3f} |\n")
            
            f.write("\n## Statistical Analysis\n\n")
            
            # Effect sizes
            f.write("### Effect Sizes (Cohen's d)\n\n")
            for metric in ['val_auc', 'val_ap', 'val_acc']:
                cohen_d = stat_comparison[metric]['cohen_d']
                f.write(f"- {metric}: {cohen_d:.3f} ")
                if abs(cohen_d) < 0.2:
                    f.write("(negligible effect)\n")
                elif abs(cohen_d) < 0.5:
                    f.write("(small effect)\n")
                elif abs(cohen_d) < 0.8:
                    f.write("(medium effect)\n")
                else:
                    f.write("(large effect)\n")
            
            f.write("\n## Recommendations\n\n")
            
            # Determine winner based on test set AUC
            if imbalanced_final['test_metrics']['auc'] > balanced_final['test_metrics']['auc']:
                f.write("**Recommended Approach: IMBALANCED**\n\n")
                f.write("The imbalanced approach shows superior performance on the held-out test set ")
                f.write("and better reflects real-world deployment conditions.\n")
            else:
                f.write("**Recommended Approach: BALANCED**\n\n")
                f.write("The balanced approach shows superior performance on the held-out test set ")
                f.write("despite having less training data.\n")
            
            f.write("\n## Methodological Advantages\n\n")
            f.write("This analysis addresses common pitfalls in ML model comparison:\n\n")
            f.write("1. **No data leakage**: Test set never used during model selection\n")
            f.write("2. **No overfitting to validation set**: Nested CV prevents this\n")
            f.write("3. **Statistical rigor**: Significance testing and effect sizes\n")
            f.write("4. **Honest performance**: Test set results are unbiased estimates\n")
            
        print(f"üìù Report saved: {report_file}")

    def save_final_test_sets(self):
        """
        Save the final held-out test sets for both balanced and imbalanced datasets to disk
        after splitting with fixed random_state=42.
        """
        import pandas as pd, pathlib, json

        pathlib.Path(self.output_dir).mkdir(parents=True, exist_ok=True)

        # Save balanced test set
        balanced_df = pd.DataFrame(self.balanced_test)
        balanced_file = f"{self.output_dir}/balanced_testset_{self.timestamp}.csv"
        balanced_df.to_csv(balanced_file, index=False)
        print(f"üíæ Balanced test set saved to: {balanced_file}")

        # Save imbalanced test set
        imbalanced_df = pd.DataFrame(self.imbalanced_test)
        imbalanced_file = f"{self.output_dir}/imbalanced_testset_{self.timestamp}.csv"
        imbalanced_df.to_csv(imbalanced_file, index=False)
        print(f"üíæ Imbalanced test set saved to: {imbalanced_file}")

    
    def create_robust_visualizations(self, nested_cv_results, stat_comparison,
                                   balanced_final, imbalanced_final):
        """
        Create comprehensive visualizations for the robust comparison.
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # Plot 1: Nested CV AUC scores by fold
        bal_aucs = nested_cv_results['balanced']['outer_scores']['val_auc']
        imb_aucs = nested_cv_results['imbalanced']['outer_scores']['val_auc']
        
        folds = range(1, len(bal_aucs) + 1)
        axes[0, 0].plot(folds, bal_aucs, 'o-', label='Balanced', linewidth=2, markersize=8)
        axes[0, 0].plot(folds, imb_aucs, 's-', label='Imbalanced', linewidth=2, markersize=8)
        axes[0, 0].set_xlabel('Outer CV Fold')
        axes[0, 0].set_ylabel('Validation AUC')
        axes[0, 0].set_title('Nested CV: AUC by Fold')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Plot 2: Statistical comparison with confidence intervals
        metrics = ['AUC', 'AP', 'Accuracy']
        metric_keys = ['val_auc', 'val_ap', 'val_acc']
        
        x = np.arange(len(metrics))
        width = 0.35
        
        bal_means = [nested_cv_results['balanced']['metrics'][m]['mean'] for m in metric_keys]
        bal_stds = [nested_cv_results['balanced']['metrics'][m]['std'] for m in metric_keys]
        imb_means = [nested_cv_results['imbalanced']['metrics'][m]['mean'] for m in metric_keys]
        imb_stds = [nested_cv_results['imbalanced']['metrics'][m]['std'] for m in metric_keys]
        
        # Calculate 95% confidence intervals
        n_folds = 5
        bal_ci = [1.96 * s / np.sqrt(n_folds) for s in bal_stds]
        imb_ci = [1.96 * s / np.sqrt(n_folds) for s in imb_stds]
        
        axes[0, 1].bar(x - width/2, bal_means, width, yerr=bal_ci, 
                      label='Balanced', alpha=0.8, capsize=5)
        axes[0, 1].bar(x + width/2, imb_means, width, yerr=imb_ci,
                      label='Imbalanced', alpha=0.8, capsize=5)
        
        # Add significance stars
        for i, (metric_key, metric_name) in enumerate(zip(metric_keys, metrics)):
            if stat_comparison[metric_key]['significant']:
                y_max = max(bal_means[i] + bal_ci[i], imb_means[i] + imb_ci[i])
                axes[0, 1].text(i, y_max + 0.01, '*', ha='center', fontsize=14)
        
        axes[0, 1].set_xlabel('Metrics')
        axes[0, 1].set_ylabel('Score (95% CI)')
        axes[0, 1].set_title('Nested CV Performance Comparison')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(metrics)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Plot 3: Test set ROC curves
        from sklearn.metrics import roc_curve
        
        # Get predictions for ROC curves
        bal_features = self.engineer_features(self.balanced_test)
        imb_features = self.engineer_features(self.imbalanced_test)

        # the exact column list the final model was trained on
        model_feats = balanced_final['feature_importance']['feature'].tolist()

        # align both DataFrames to that column set
        bal_features = bal_features.reindex(columns=model_feats + ['label', 'script_id'],
                                            fill_value=0)
        imb_features = imb_features.reindex(columns=model_feats + ['label', 'script_id'],
                                        fill_value=0)

        
        bal_X = bal_features[model_feats].values
        bal_y = bal_features['label'].values
        bal_proba = balanced_final['model'].predict_proba(bal_X)[:, 1]
        
        imb_X = imb_features[model_feats].values
        imb_y = imb_features['label'].values
        imb_proba = imbalanced_final['model'].predict_proba(imb_X)[:, 1]
        
        bal_fpr, bal_tpr, _ = roc_curve(bal_y, bal_proba)
        imb_fpr, imb_tpr, _ = roc_curve(imb_y, imb_proba)
        
        axes[0, 2].plot(bal_fpr, bal_tpr, 
                       label=f'Balanced (AUC={balanced_final["test_metrics"]["auc"]:.3f})',
                       linewidth=2)
        axes[0, 2].plot(imb_fpr, imb_tpr,
                       label=f'Imbalanced (AUC={imbalanced_final["test_metrics"]["auc"]:.3f})',
                       linewidth=2)
        axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5)
        axes[0, 2].set_xlabel('False Positive Rate')
        axes[0, 2].set_ylabel('True Positive Rate')
        axes[0, 2].set_title('Test Set ROC Curves')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # Plot 4: Feature importance comparison
        top_n = 10
        bal_imp = balanced_final['feature_importance'].head(top_n)
        imb_imp = imbalanced_final['feature_importance'].head(top_n)
        
        # Find common features
        common_features = set(bal_imp['feature']).intersection(set(imb_imp['feature']))
        common_features = list(common_features)[:8]
        
        if common_features:
            bal_values = [bal_imp[bal_imp['feature'] == f]['importance'].iloc[0] 
                         for f in common_features]
            imb_values = [imb_imp[imb_imp['feature'] == f]['importance'].iloc[0] 
                         for f in common_features]
            
            y_pos = np.arange(len(common_features))
            axes[1, 0].barh(y_pos - 0.2, bal_values, 0.4, label='Balanced', alpha=0.8)
            axes[1, 0].barh(y_pos + 0.2, imb_values, 0.4, label='Imbalanced', alpha=0.8)
            axes[1, 0].set_yticks(y_pos)
            axes[1, 0].set_yticklabels([f[:20] + '...' if len(f) > 20 else f 
                                        for f in common_features], fontsize=9)
            axes[1, 0].set_xlabel('Feature Importance')
            axes[1, 0].set_title('Top Feature Importance Comparison')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        
        # Plot 5: Effect sizes visualization
        metrics_display = ['AUC', 'AP', 'Accuracy']
        cohen_ds = [stat_comparison[m]['cohen_d'] for m in metric_keys]
        
        colors = ['green' if abs(d) < 0.2 else 'yellow' if abs(d) < 0.5 
                 else 'orange' if abs(d) < 0.8 else 'red' for d in cohen_ds]
        
        axes[1, 1].bar(metrics_display, cohen_ds, color=colors, alpha=0.7)
        axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        axes[1, 1].axhline(y=0.2, color='gray', linestyle='--', alpha=0.5)
        axes[1, 1].axhline(y=-0.2, color='gray', linestyle='--', alpha=0.5)
        axes[1, 1].set_ylabel("Cohen's d")
        axes[1, 1].set_title('Effect Sizes (Imbalanced - Balanced)')
        axes[1, 1].grid(True, alpha=0.3)
        
        # Plot 6: Test set confusion matrices
        from sklearn.metrics import confusion_matrix
        
        bal_pred = balanced_final['model'].predict(bal_X)
        imb_pred = imbalanced_final['model'].predict(imb_X)
        
        bal_cm = confusion_matrix(bal_y, bal_pred)
        imb_cm = confusion_matrix(imb_y, imb_pred)
        
        # Normalize confusion matrices
        bal_cm_norm = bal_cm.astype('float') / bal_cm.sum(axis=1)[:, np.newaxis]
        imb_cm_norm = imb_cm.astype('float') / imb_cm.sum(axis=1)[:, np.newaxis]
        
        # Create subplots for confusion matrices
        im1 = axes[1, 2].imshow(bal_cm_norm, cmap='Blues', vmin=0, vmax=1)
        axes[1, 2].set_title('Balanced Approach\nTest Set Confusion Matrix')
        axes[1, 2].set_xlabel('Predicted')
        axes[1, 2].set_ylabel('Actual')
        
        # Add text annotations
        for i in range(2):
            for j in range(2):
                text = axes[1, 2].text(j, i, f'{bal_cm[i, j]}\n({bal_cm_norm[i, j]:.2f})',
                                     ha='center', va='center')
        
        plt.suptitle('Robust Model Comparison: Nested Cross-Validation Results', 
                    fontsize=16)
        plt.tight_layout()
        
        # Save the plot
        plot_file = f"{self.output_dir}/robust_comparison_{self.timestamp}.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìä Visualization saved: {plot_file}")


# Example usage
if __name__ == "__main__":
    print("üöÄ Starting Robust Model Comparison with Nested Cross-Validation...")
    
    # Initialize the robust comparison system
    comparator = RobustBalancedVsImbalancedComparison(
        output_dir="robust_comparison_results"
    )
    
    try:
        # Run the robust comparison
        results = comparator.run_robust_comparison()
        
        print("\n‚úÖ Robust comparison complete!")
        print("\nüéØ Key Takeaways:")
        print("1. Used nested cross-validation to avoid hyperparameter selection bias")
        print("2. Evaluated on completely held-out test set")
        print("3. Performed statistical significance testing")
        print("4. Results are methodologically sound and defensible for thesis")
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()